---
title: "R Notebook"
output: html_notebook
---


Trend analysis of leatherback turtles nesting in Florida with Kelly Stewart.

New data were received in August 2020. 

```{r}
rm(list=ls())
library(tidyverse)
library(ggplot2)
library(readr)
library(lubridate)
library(jagsUI)
library(bayesplot)
library(stringr)
save.fig <- T

source("Caribbean_Dc_fcns.R")

MCMC.params <- list(n.samples = 75000,
                    n.burnin = 55000,
                    n.thin = 5,
                    n.chains = 5)

n.per.chain <- (MCMC.params$n.samples - MCMC.params$n.burnin)/MCMC.params$n.thin

```

Get data;

```{r}
col.def <- cols(ID = col_integer(),
                year = col_integer(),
                beach = col_character(),
                latitude = col_double(),
                distance = col_double(),
                days_week = col_integer(),
                days_year = col_integer(),
                nests = col_integer())

FL.nest.counts <- read_csv("data/FL_Sept2020.csv", 
                           col_types = col.def) %>% 
  mutate(beach_f = as.factor(toupper(beach)),
         dataset = "FL",
         ID2 = as.numeric(as.factor(ID)))


```

Change in abundance over time at each beach can be seen in FL_PR_STX_v0_Sept2020.Rmd. 

For FL, we can fit the same models as the last publication. I use the same set of covariates but also test all possibilities of 3 covariates to see which set is the best.  

```{r}

FL.nest.counts %>% 
  group_by(ID2) %>%
  summarise(n = n()) -> FL.ns

FL.nest.counts %>% 
  select(ID2, ID, latitude, beach_f, year) %>% 
  group_by(ID) %>%
  summarise(ID2 = first(ID2),
            name = first(beach_f),
            latitude = first(latitude),
            median.yr = median(year),
            min.yr = min(year),
            n = n()) %>%
  mutate(latc = latitude - mean(latitude),
         latc2 = latc^2,
         beach = ID,
         ID2 = ID2,
         name = name) -> FL.lat.dat

median.yr <- select(FL.lat.dat, c(ID, median.yr, min.yr)) %>%
  right_join(FL.nest.counts, by = "ID") %>% 
  select(ID, median.yr, min.yr, year)

# changed from centering to left-adjusted for years. I think t >= 0 makes more
# sense explaining the intercept than centering time. With t >= 0, it is the 
# log(mu) at time 0 for each beach, whereas if time is centered, it is log(mu) at
# some arbitrary year for each beach. 

FL.jags.data <- list(N = length(FL.nest.counts$nests),
                     nbeach = length(unique(FL.nest.counts$ID2)),
                     count = FL.nest.counts$nests,
                     beach = FL.nest.counts$ID2,
                     yearc = median.yr$year - median.yr$min.yr,
                     latc = FL.lat.dat$latc,
                     latc2 = FL.lat.dat$latc2)

data.vector <- FL.jags.data$count %>% 
  rep(each = MCMC.params$n.chains * n.per.chain)

parameters <- c("a0", "a1", "beta", "B.hat", "a1pc",
                "mu.a0", "mu.a1", "mu",
                "rea0", "rea1",
                "delta1", "delta2",
                "sigma.e", "Sigma.B",
                "sigma.a0", "sigma.a1",
                "floridapc", "probP", "deviance",
                "Devobs", "Devpred", "loglik")

```


```{r}
# reduced set of models as of Sept 2020. I can't remember why I removed
# "beach" as a covariate... 
# Found this comment "Looks like some parameters/models converged fine. Those with beach-specific betas didn't converge well."
# I think that's the reason the beach covariate was removed. 

model.names <- c("3Covs", rep("2Covs", times = 3), 
                 rep("1Cov", times = 3), "0Cov")
out.names <- c("3Covs", "logD_DayWk", "logD_DayYr", "DayWk_DayYr",
               "logD", "DayWk", "DayYr", "0Cov") 

# model.names <- c("3Covs", "3Covs_beach", 
#                  rep("2Covs", times = 3),
#                  rep("2Covs_beach", times = 3),
#                  rep("1Cov", times = 3),
#                  rep("1Cov_beach", times = 3),"0Cov")
# out.names <- c("3Covs", "3Covs_beach", 
#                "logD_DayWk", "logD_DayYr", "DayWk_DayYr",
#                "logD_DayWk_beach", "logD_DayYr_beach", "DayWk_DayYr_beach",
#                "logD", "DayWk", "DayYr", 
#                "logD_beach", "DayWk_beach", "DayYr_beach", "0Cov") 
X.FL <- list(cbind(log(FL.nest.counts$distance) -
                     median(log(FL.nest.counts$distance)),
                   FL.nest.counts$days_week -
                     median(FL.nest.counts$days_week),
                   FL.nest.counts$days_year -
                     median(FL.nest.counts$days_year)),
             cbind(log(FL.nest.counts$distance) -
                     median(log(FL.nest.counts$distance)),
                   FL.nest.counts$days_week -
                     median(FL.nest.counts$days_week)),
             cbind(log(FL.nest.counts$distance) -
                     median(log(FL.nest.counts$distance)),
                   FL.nest.counts$days_year -
                     median(FL.nest.counts$days_year)),
             cbind(FL.nest.counts$days_week -
                     median(FL.nest.counts$days_week),
                   FL.nest.counts$days_year -
                     median(FL.nest.counts$days_year)),
             log(FL.nest.counts$distance) -
               median(log(FL.nest.counts$distance)),
             FL.nest.counts$days_week - 
               median(FL.nest.counts$days_week),
             FL.nest.counts$days_year - 
               median(FL.nest.counts$days_year),
             0)

#jm <- list(length = length(out.names))
loo.out <- list()
Rmax <- vector(mode = "numeric", length = length(out.names))

for (k in 1:length(out.names)){
  MCMC.params$model.file = paste0("models/Model_JAGS_rSlope_rInt_",
                                  model.names[k], ".txt")
  FL.jags.data$X <- X.FL[[k]]

  if (!file.exists(paste0("RData/JAGS_out_rSlope_rInt_", 
                          out.names[k], "_FL.rds"))){
    jm <- jags(data = FL.jags.data,
               #inits = inits,
               parameters.to.save= parameters,
               model.file = MCMC.params$model.file,
               n.chains = MCMC.params$n.chains,
               n.burnin = MCMC.params$n.burnin,
               n.thin = MCMC.params$n.thin,
               n.iter = MCMC.params$n.samples,
               DIC = T, 
               parallel=T)
    
    saveRDS(jm, 
            file = paste0("RData/JAGS_out_rSlope_rInt_", 
                          out.names[k], "_FL.rds"))
    
  } else {
    jm <- readRDS(file = paste0("RData/JAGS_out_rSlope_rInt_",
                                out.names[k], "_FL.rds"))
  }

  Rmax[k] <- max(unlist(lapply(jm$Rhat, FUN = max)))
  
  if (!file.exists(paste0("RData/LOOIC_rSlope_rInt_", 
                          out.names[k], "_FL.rds"))){
    loo.out[[k]] <- compute.LOOIC(loglik = jm$sims.list$loglik, 
                                  data.vector = FL.jags.data$count, 
                                  MCMC.params = MCMC.params)
    saveRDS(loo.out[[k]], 
            file = paste0("RData/LOOIC_rSlope_rInt_",
                          out.names[k], "_FL.rds"))
    
  } else {
    loo.out[[k]] <- readRDS(file = paste0("RData/LOOIC_rSlope_rInt_",
                                          out.names[k], "_FL.rds"))
  }
  
}

loo.all <- lapply(loo.out, FUN = function(X) X$loo.out) %>%
  lapply(FUN = function(X){ 
    X$estimates %>% 
      data.frame() %>% 
      rownames_to_column(var = "Statistic")}) %>%
  lapply(FUN = function(X) filter(X, Statistic == "looic") 
         %>% select(Estimate)) %>%
  unlist()

looic.df.Poi <- data.frame(ID = seq(1, length(model.names)),
                           model = model.names,
                           cov = out.names,
                           looic = loo.all,
                           MaxRhat = Rmax) %>%
  arrange(-desc(looic))



# summary(jm.FL.3Covs)
# rm(jm.FL.3Covs)
```

Then... use LOOIC or DIC to compare these models. 

```{r}
looic.df.Poi
```


Looks like the 3 covariate model is the best. Pareto k statistic isn't great though... About 34% were > 0.7. Should we be concerned? Not really... 

```{r}
sum(loo.out[[looic.df.Poi[1,"ID"]]]$loo.out$diagnostics$pareto_k > 0.7)/length(FL.jags.data$count)
```


Take a look at which ones are bad ones

```{r}
idx.bad.looic <- loo.out[[looic.df.Poi[1,"ID"]]]$loo.out$diagnostics$pareto_k > 0.7

FL.paretok.data <- data.frame(beach = FL.jags.data$beach,
                            count = FL.jags.data$count,
                            yearc = FL.jags.data$yearc,
                            year = FL.nest.counts$year,
                            pareto_k = loo.out[[looic.df.Poi[1,"ID"]]]$loo.out$diagnostics$pareto_k)

ggplot(data = FL.paretok.data) +
  geom_point(aes(x = beach, 
                 y = yearc, 
                 color = pareto_k)) +
  scale_color_continuous(type = "viridis")
```

Some beaches are not good throughout their data, whereas others are okay... This means that we may need a different model to accommodate all... 

Take a look at residuals
```{r}
jm <- readRDS(file = paste0("RData/JAGS_out_rSlope_rInt_", 
                            out.names[looic.df.Poi[1,"ID"]], "_FL.rds"))

summary.df <- jm$summary %>% 
  data.frame() %>% 
  rownames_to_column(var = "Parameter")

# regular expression - ^a0 means the beginning of a string has to be a0
# \\[ are the escape characters for [, which is a reserved character. 
# ([0-9]+) means numbers 
# \\] also to literal ]
summary.a0.df <- summary.df[grep(pattern = "^a0\\[([0-9]+)\\]", 
                                 x = summary.df$Parameter),] %>%
  rownames_to_column("beach") %>% mutate(ID = as.numeric(beach))
summary.a1.df <- summary.df[grep(pattern = "^a1\\[([0-9]+)\\]", 
                                 x = summary.df$Parameter),]

#grep(pattern = ".+\\[([0-9]+)\\].+?$", x = summary.df$Parameter)

# summary.mu.df <- summary.df[grep(pattern = "^mu\\[([0-9]+)\\]", 
#                                  x = summary.df$Parameter),] %>%
#   mutate(beach = FL.jags.data$beach)

summary.beta1.df <- summary.df[grep(pattern = "beta[1]", 
                                    x = summary.df$Parameter, 
                                    fixed = TRUE),]
summary.beta2.df <- summary.df[grep(pattern = "beta[2]", 
                                    x = summary.df$Parameter, 
                                    fixed = TRUE),]
summary.beta3.df <- summary.df[grep(pattern = "beta[3]", 
                                    x = summary.df$Parameter, 
                                    fixed = TRUE),]

resid <- log.mu <- vector(mode = "numeric", 
                          length = length(FL.jags.data$count))
beach <- FL.jags.data$beach
yearc <- FL.jags.data$yearc
X <- X.FL[[1]]
for (i in 1:length(FL.jags.data$count)){
  log.mu[i] <- summary.a0.df[beach[i], "mean"] + 
    summary.a1.df[beach[i], "mean"] * yearc[i] + 
    summary.beta1.df[1, "mean"] * X[i,1] + 
    summary.beta2.df[1, "mean"] * X[i,2] + 
    summary.beta3.df[1, "mean"] * X[i,3]
  
  resid[i] <- FL.jags.data$count[i] - exp(log.mu[i])
}

summary.loglik.df <- summary.df %>% 
  filter(str_detect(Parameter, "loglik"))
                                           
FL.paretok.data$residual <- resid
FL.paretok.data$mu <- exp(log.mu)
FL.paretok.data$loglik <- summary.loglik.df$mean

# remove pareto_k < 0.7 because they are supposedly okay.

FL.paretok.data %>% filter(pareto_k > 0.7) -> FL.paretok.data.2
ggplot(data = FL.paretok.data.2) +
  geom_point(aes(x = beach, 
                 y = yearc,
                 color = abs(residual)))+
  scale_color_continuous(type = "viridis")

```
Only a few beaches have horrible residuals. Is there a relationship between residuals and  Pareto k?

```{r}

ggplot(data = FL.paretok.data.2) +
  geom_point(aes(x = residual, 
                 y =  pareto_k)) 
#  scale_color_continuous(type = "viridis")

```
Not really a relationship there... 


```{r}

ggplot(data = FL.paretok.data.2) +
  geom_point(aes(x = count, 
                 y =  mu,
                 color = pareto_k))  +
  scale_color_continuous(type = "viridis")

```

There is no particular pattern between Pareto k and observed count or estimated mu. 

How can I make the model more flexible? 

I tried the negative binomial distribution in a couple ways instead of Poisson but it didn't work as expected. I might have used the distribution incorrectly. Kelly wanted to use the same model as before so let's move on for now. 

For STX, I used log-normal distribution and they worked better than the Poisson models. So, I'll give that a try for FL data.


```{r}
loo.out <- list()
Rmax <- vector(mode = "numeric", length = length(out.names))
FL.jags.data$log.count <- log(FL.jags.data$count + 1)

for (k in 1:length(out.names)){
  MCMC.params$model.file = paste0("models/Model_JAGS_LogNorm_rSlope_rInt_",
                                  model.names[k], ".txt")
  FL.jags.data$X <- X.FL[[k]]

  if (!file.exists(paste0("RData/JAGS_out_LogNorm_rSlope_rInt_", 
                          out.names[k], "_FL.rds"))){
    jm <- jags(data = FL.jags.data,
               #inits = inits,
               parameters.to.save= parameters,
               model.file = MCMC.params$model.file,
               n.chains = MCMC.params$n.chains,
               n.burnin = MCMC.params$n.burnin,
               n.thin = MCMC.params$n.thin,
               n.iter = MCMC.params$n.samples,
               DIC = T, 
               parallel=T)
    
    saveRDS(jm, 
            file = paste0("RData/JAGS_out_LogNorm_rSlope_rInt_", 
                          out.names[k], "_FL.rds"))
    
  } else {
    jm <- readRDS(file = paste0("RData/JAGS_out_LogNorm_rSlope_rInt_",
                                out.names[k], "_FL.rds"))
  }

  Rmax[k] <- max(unlist(lapply(jm$Rhat, FUN = max)))
  
  if (!file.exists(paste0("RData/LOOIC_LogNorm_rSlope_rInt_", 
                          out.names[k], "_FL.rds"))){
    loo.out[[k]] <- compute.LOOIC(loglik = jm$sims.list$loglik, 
                                  data.vector = FL.jags.data$count, 
                                  MCMC.params = MCMC.params)
    saveRDS(loo.out[[k]], 
            file = paste0("RData/LOOIC_LogNorm_rSlope_rInt_",
                          out.names[k], "_FL.rds"))
    
  } else {
    loo.out[[k]] <- readRDS(file = paste0("RData/LOOIC_LogNorm_rSlope_rInt_",
                                          out.names[k], "_FL.rds"))
  }
  
}

loo.all <- lapply(loo.out, FUN = function(X) X$loo.out) %>%
  lapply(FUN = function(X){ 
    X$estimates %>% 
      data.frame() %>% 
      rownames_to_column(var = "Statistic")}) %>%
  lapply(FUN = function(X) filter(X, Statistic == "looic") 
         %>% select(Estimate)) %>%
  unlist()

looic.df.Norm <- data.frame(ID = seq(1, length(model.names)),
                           model = model.names,
                           cov = out.names,
                           looic = loo.all,
                           MaxRhat = Rmax) %>%
  arrange(-desc(looic))


```

```{r}
looic.df.Norm
```

None of these converged... 

Stick with the Poisson models. 


```{r}
jm <- readRDS(file = paste0("RData/JAGS_out_rSlope_rInt_", 
                            out.names[looic.df.Poi[1,"ID"]], "_FL.rds"))

summary.df <- jm$summary %>% 
  data.frame() %>% 
  rownames_to_column(var = "Parameter")

# regular expression - ^a0 means the beginning of a string has to be a0
# \\[ are the escape characters for [, which is a reserved character. 
# ([0-9]+) means numbers 
# \\] also to literal ]
summary.a0.df <- summary.df[grep(pattern = "^a0\\[([0-9]+)\\]", 
                                 x = summary.df$Parameter),] %>%
  rownames_to_column("beach") %>% mutate(ID = as.numeric(beach))
summary.a1.df <- summary.df[grep(pattern = "^a1\\[([0-9]+)\\]", 
                                 x = summary.df$Parameter),]

#grep(pattern = ".+\\[([0-9]+)\\].+?$", x = summary.df$Parameter)

# summary.mu.df <- summary.df[grep(pattern = "^mu\\[([0-9]+)\\]",
#                                  x = summary.df$Parameter),] %>%
#   mutate(beach = FL.jags.data$beach)

summary.beta1.df <- summary.df[grep(pattern = "beta[1]", 
                                    x = summary.df$Parameter, 
                                    fixed = TRUE),]
summary.beta2.df <- summary.df[grep(pattern = "beta[2]", 
                                    x = summary.df$Parameter, 
                                    fixed = TRUE),]
summary.beta3.df <- summary.df[grep(pattern = "beta[3]", 
                                    x = summary.df$Parameter, 
                                    fixed = TRUE),]

mcmc_trace(jm$samples, c("beta[1]", "beta[2]", "beta[3]" ))
```

These look okay. 

Take a look at the over-dispersion variance parameter.

```{r}
mcmc_dens(jm$samples, pars = "sigma.e")
```

It's a little greater than 0, but not much... So, we can ignore overdispersion?

Extract population growth parameters. 

```{r}
FL.summary.df <- data.frame(jm$summary) %>%
  rownames_to_column("Parameter")

FL.summary.df %>% 
  filter(str_detect(Parameter, pattern = "a1pc")) %>%
  rownames_to_column("beach") %>% #-> tmp #
  transmute(ID = as.numeric(beach),
            mean_a1pc = mean,
            sd_a1pc = sd,
            lower25_a1pc = X2.5.,
            median_a1pc = X50.,
            upper975_a1pc = X97.5.)-> FL.a1pc

FL.a1pc %>% 
  left_join(FL.lat.dat, by = "ID") %>%
  arrange(ID) -> FL.a1pc

FL.summary.df %>% 
  filter(str_detect(Parameter, pattern = "mu.a1")) %>%
  rownames_to_column("beach") %>%
  transmute(ID = as.numeric(beach),
            mean_mu.a1 = mean,
            sd_mu.a1 = sd,
            lower25_mu.a1 = X2.5.,
            median_mu.a1 = X50.,
            upper975_mu.a1 = X97.5.)-> FL.mu.a1

FL.mu.a1 %>% left_join(FL.lat.dat, by = "ID") %>%
  arrange(ID) -> FL.mu.a1

#head(a1pc.2008)
```


Look at the beach specific annual growth rate in %:

```{r}

ggplot() + 
  geom_point(data = FL.a1pc, 
             aes(x = beach, y = mean_a1pc,
             color = latc)) + 
  geom_errorbar(data = FL.a1pc,
                aes(ymin = lower25_a1pc,
                    ymax = upper975_a1pc,
                    x = beach,
                    color = latc)) + 
  ylab("Annual average growth rate (%) and 95% CI") + 
  xlab("Beach ID")
```

Create a table with beach names:

```{r}
FL.a1pc %>% select(ID, name, 
                   mean_a1pc, sd_a1pc, 
                   lower25_a1pc, upper975_a1pc) %>%
  rename(ID = ID, Beach = name, 
         mean = mean_a1pc, SD = sd_a1pc,
         lower_2.5 = lower25_a1pc,
         upper_97.5 = upper975_a1pc) -> FL.table

#write_csv(FL.table, path = "data/FL_growth_rates_Sept2020.csv")
```

Visualize FL output to see if they make sense.

```{r}
# For each beach, extract data, plot, then overlay predicted growth.
# Need to use samples from joint posteriors 
# The model

# count[i] ~ dpois(mu[i])
# log(mu[i]) <- a0[beach[i]] + a1[beach[i]] * yearc[i] + beta[1] * X[i,1] + beta[2] * X[i,2] + beta[3] * X[i,3] + epsilon[i]

beta.1.samples <- extract.samples("beta[1]", jm$samples)
beta.2.samples <- extract.samples("beta[2]", jm$samples)
beta.3.samples <- extract.samples("beta[3]", jm$samples)

# beta.1 <- summary.beta1.df$mean
# beta.2 <- summary.beta2.df$mean
# beta.3 <- summary.beta3.df$mean
# Beta <- c(beta.1, beta.2, beta.3)
k <- 57
for (k in 1:FL.jags.data$nbeach){
  line.df <- data.frame(Year = as.numeric(),
                        Mean = as.numeric(),
                        lower2.5 = as.numeric(),
                        upper97.5 = as.numeric())

  nest.counts.beach <- FL.nest.counts %>% filter(ID == k)
  # slope
  a1.samples <- extract.samples(paste0("a1[", k, "]"), jm$samples)
  # a1.tmp <- FL.mu.a1 %>% 
  #   filter(ID == k) %>% 
  #   select(mean_mu.a1)
  
  # intercept
  a0.samples <- extract.samples(paste0("a0[", k, "]"), jm$samples)
  
  #a0.tmp <- summary.a0.df %>% filter(ID == k) %>% select(mean)
  
  # covariates
  X.tmp <- X[FL.nest.counts$ID == k,]
  
  # years
  yr.tmp <- nest.counts.beach$year - min(nest.counts.beach$year)
  
  for (t in 0:(nrow(nest.counts.beach)-1)){
    Y <- yr.tmp[t+1] + min(nest.counts.beach$year)

    samples <- a0.samples + 
      a1.samples * yr.tmp[t+1] + 
      beta.1.samples * X.tmp[t+1,1] + 
      beta.2.samples * X.tmp[t+1,2] + 
      beta.3.samples * X.tmp[t+1,3] 
    
    line.df[t+1,] <- c(Y, mean(samples), 
                       quantile(samples, c(0.025,0.975)))  
  }
  
  ggplot() +
    geom_point(data = nest.counts.beach,
               aes(x = year, y = nests),
               color = "red") +
    geom_path(data = nest.counts.beach,
              aes(x = year, y = nests),
              color = "red") +
    
    geom_ribbon(data = line.df, 
              aes(x = Year, 
                  ymin = exp(lower2.5),
                  ymax = exp(upper97.5)),
              fill = "orange", alpha = 0.4) +
    geom_path(data = line.df, 
              aes(x = Year, y = exp(Mean)),
              size = 1.2) +
    labs(title = nest.counts.beach$beach[1]) +
    ylab("# nests") + xlab("")

  beach.name <- str_replace(nest.counts.beach$beach[1], "/", "_")

  if (save.fig)
    ggsave(filename = paste0("figures/", beach.name, ".png"),
           device = "png", dpi = 600)

}





``` 


One thing I noticed at this point is that many have the same pattern where the nest counts started to decline around 2010. 

As I did for the STX dataset, it may be good to add a change-point parameter. 

```{r}
parameters <- c("a0.1", "a1.1", "a0.2", "a1.2",
                "beta", "B.1.hat", "B.2.hat", 
                "delta1", "delta2", "year.change",
                "sigma.e", "Sigma.B.1", "Sigma.B.2",
                "sigma.a0.1", "sigma.a1.1", 
                "sigma.a0.2", "sigma.a1.2", "loglik")

FL.jags.data.2slopes <- list(N = length(FL.nest.counts$nests),
                             nbeach = length(unique(FL.nest.counts$ID2)),
                             count = FL.nest.counts$nests,
                             beach = FL.nest.counts$ID2,
                             yearc = median.yr$year - min(median.yr$min.yr),
                             latc = FL.lat.dat$latc,
                             latc2 = FL.lat.dat$latc2,
                             minT = 15,
                             maxT = 35)


# For these models, years have to be the same across the board because 
# I assume the change point is happening the same at all beaches

#jm <- list(length = length(out.names))
loo.out <- list()
Rmax <- vector(mode = "numeric", length = length(out.names))
k <- 1
for (k in 1:length(out.names)){
  MCMC.params$model.file = paste0("models/Model_JAGS_r2Slopes_rInt_",
                                  model.names[k], ".txt")
  FL.jags.data.2slopes$X <- X.FL[[k]]

  if (!file.exists(paste0("RData/JAGS_out_r2Slopes_rInt_", 
                          out.names[k], "_FL.rds"))){
    jm <- jags(data = FL.jags.data.2slopes,
               #inits = inits,
               parameters.to.save= parameters,
               model.file = MCMC.params$model.file,
               n.chains = MCMC.params$n.chains,
               n.burnin = MCMC.params$n.burnin,
               n.thin = MCMC.params$n.thin,
               n.iter = MCMC.params$n.samples,
               DIC = T, 
               parallel=T)
    
    saveRDS(jm, 
            file = paste0("RData/JAGS_out_r2Slopes_rInt_", 
                          out.names[k], "_FL.rds"))
    
  } else {
    jm <- readRDS(file = paste0("RData/JAGS_out_r2Slopes_rInt_",
                                out.names[k], "_FL.rds"))
  }

  Rmax[k] <- max(unlist(lapply(jm$Rhat, FUN = max)))
  
  if (!file.exists(paste0("RData/LOOIC_r2Slopes_rInt_", 
                          out.names[k], "_FL.rds"))){
    loo.out[[k]] <- compute.LOOIC(loglik = jm$sims.list$loglik, 
                                  data.vector = FL.jags.data$count, 
                                  MCMC.params = MCMC.params)
    saveRDS(loo.out[[k]], 
            file = paste0("RData/LOOIC_r2Slopes_rInt_",
                          out.names[k], "_FL.rds"))
    
  } else {
    loo.out[[k]] <- readRDS(file = paste0("RData/LOOIC_r2Slopes_rInt_",
                                          out.names[k], "_FL.rds"))
  }
  
}

loo.all <- lapply(loo.out, FUN = function(X) X$loo.out) %>%
  lapply(FUN = function(X){ 
    X$estimates %>% 
      data.frame() %>% 
      rownames_to_column(var = "Statistic")}) %>%
  lapply(FUN = function(X) filter(X, Statistic == "looic") 
         %>% select(Estimate)) %>%
  unlist()

looic.df.Poi.2slopes <- data.frame(ID = seq(1, length(model.names)),
                                   model = model.names,
                                   cov = out.names,
                                   looic = loo.all,
                                   MaxRhat = Rmax) %>%
  arrange(-desc(looic))




```


<!-- Or use the estimated mu directly: -->

<!-- ```{r} -->

<!-- # For each beach, extract data, plot, then overlay predicted growth. -->
<!-- beach.plots <- list() -->
<!-- k <- 3 -->
<!-- for (k in 1:FL.jags.data$nbeach){ -->
<!--   tmp <- FL.nest.counts %>% filter(ID == k) -->

<!--   summary.mu.df %>% filter(beach == k) %>%  -->
<!--     select(mean) -> mu.tmp -->

<!--   tmp$mu <- mu.tmp$mean -->
<!--   beach.plots[[k]] <- ggplot(data = tmp) + -->
<!--     geom_point(aes(x = year, y = nests)) +  -->
<!--     geom_path(aes(x = year, y = mu)) + -->
<!--     labs(title = tmp$beach[1]) +  -->
<!--     ylab("# nests") + xlab("") -->

<!--   beach.name <- str_replace(tmp$beach[1], "/", "_") -->

<!--   if (save.fig) -->
<!--     ggsave(filename = paste0("figures/", beach.name, "_mu.png"), -->
<!--            device = "png", dpi = 600) -->
<!-- } -->


<!-- ``` -->


