---
title: "FL_v2_Dec2021; Site-specific 2 slope models for FL"
output: html_notebook
---

What if we have site-specific change in slopes? Some locations don't have sufficient data to make two slopes. So, I need to filter out beaches that have sufficient data for this analysis. I wonder if it's worth spending that much time... 

FL_PR_STX_v0_Sept2020.Rmd contains all time series, although the plots are in the log scale. The hypothesis is that if the change in trends happened around the same time as what was found in STX. At Sandy Point, STX, the change point happened around 20-30 years since 1982 (2002 - 2012). So, we want to have time series that contains at least 5 years before (1997) and 5 years after (2017). 

New data were received in August 2020. 

```{r}
rm(list=ls())
library(tidyverse)
library(ggplot2)
library(readr)
library(lubridate)
library(jagsUI)
library(bayesplot)
library(stringr)

source("Caribbean_Dc_fcns.R")

MCMC.params <- list(n.samples = 75000,
                    n.burnin = 55000,
                    n.thin = 5,
                    n.chains = 5)

n.per.chain <- (MCMC.params$n.samples - MCMC.params$n.burnin)/MCMC.params$n.thin

```

Get data;

```{r}
col.def <- cols(ID = col_integer(),
                year = col_integer(),
                beach = col_character(),
                latitude = col_double(),
                distance = col_double(),
                days_week = col_integer(),
                days_year = col_integer(),
                nests = col_integer())

FL.nest.counts.0 <- read_csv("data/FL_Sept2020.csv", 
                           col_types = col.def) %>% 
  mutate(beach_f = as.factor(toupper(beach)),
         dataset = "FL",
         ID2 = as.numeric(as.factor(ID)))

# a dataset should contain at least from 1997 - 2017. Longer the better.
year.1 <- 1997
year.2 <- 2017
```

Change in abundance over time at each beach can be seen in FL_PR_STX_v0_Sept2020.Rmd. 

In this analysis, I tried to look at site-specific 2-slope models while keeping the other parts of the models the same, i.e., the same set of covariates but also test all possibilities of 3 covariates to see which set is the best.  

```{r}

FL.nest.counts.0 %>% 
  group_by(ID2) %>%
  summarise(n = n()) -> FL.ns

FL.nest.counts.0 %>% 
  select(ID2, ID, latitude, beach_f, year) %>% 
  group_by(ID) %>%
  summarise(ID2 = first(ID2),
            name = first(beach_f),
            latitude = first(latitude),
            median.yr = median(year),
            min.yr = min(year),
            max.yr = max(year),
            n = n()) %>%
  filter(min.yr <= year.1 & max.yr >= year.2) %>%
  rownames_to_column(var = "ID3") %>%
  mutate(latc = latitude - mean(latitude),
         latc2 = latc^2,
         beach = ID,
         ID2 = ID2,
         ID3 = as.integer(ID3),
         name = name) -> FL.lat.dat

median.yr <- select(FL.lat.dat, 
                    c(ID, ID3, median.yr, min.yr)) %>%
  left_join(FL.nest.counts.0, by = "ID") %>% 
  select(ID, ID3, median.yr, min.yr, year) %>%
  na.omit()

# changed from centering to left-adjusted for years. 
# I think t >= 0 makes more sense explaining the 
# intercept than centering time. With t >= 0, it is the 
# log(mu) at time 0 for each beach (or some year that 
# is common among all beaches = 1997), whereas if time 
# is centered, it is log(mu) at some arbitrary year 
# for each beach. 

FL.nest.counts.0 %>% filter(ID %in% FL.lat.dat$ID) -> FL.nest.counts

# min(median.yr$year) = 1979. So, minT and maxT should be 23 (2002-1979)
# and 33 (2012-1979), where the inflection seemed to happen between 
# 2002 and 2012. 
FL.jags.data <- list(N = length(FL.nest.counts$nests),
                     nbeach = length(unique(FL.nest.counts$ID2)),
                     count = FL.nest.counts$nests,
                     beach = median.yr$ID3,
                     yearc = median.yr$year - min(median.yr$year),
                     latc = FL.lat.dat$latc,
                     latc2 = FL.lat.dat$latc2,
                     minT = 23, maxT = 33)

data.vector <- FL.jags.data$count %>% 
  rep(each = MCMC.params$n.chains * n.per.chain)

parameters <- c("a0.1", "a1.1", "a0.2", "a1.2",
                "beta", "r", "year.change", "mu",
                "delta1", "delta2",
                "sigma.e", "deviance", "loglik")

```


```{r}
# reduced set of models as of Sept 2020. I can't remember why I removed
# "beach" as a covariate... 
# Found this comment "Looks like some parameters/models converged fine. Those with beach-specific betas didn't converge well."
# I think that's the reason the beach covariate was removed. 

model.names <- c("3Covs", rep("2Covs", times = 3), 
                 rep("1Cov", times = 3), "0Cov")
out.names <- c("3Covs", "logD_DayWk", "logD_DayYr", "DayWk_DayYr",
               "logD", "DayWk", "DayYr", "0Cov") 

X.FL <- list(cbind(log(FL.nest.counts$distance) -
                     median(log(FL.nest.counts$distance)),
                   FL.nest.counts$days_week -
                     median(FL.nest.counts$days_week),
                   FL.nest.counts$days_year -
                     median(FL.nest.counts$days_year)),
             cbind(log(FL.nest.counts$distance) -
                     median(log(FL.nest.counts$distance)),
                   FL.nest.counts$days_week -
                     median(FL.nest.counts$days_week)),
             cbind(log(FL.nest.counts$distance) -
                     median(log(FL.nest.counts$distance)),
                   FL.nest.counts$days_year -
                     median(FL.nest.counts$days_year)),
             cbind(FL.nest.counts$days_week -
                     median(FL.nest.counts$days_week),
                   FL.nest.counts$days_year -
                     median(FL.nest.counts$days_year)),
             log(FL.nest.counts$distance) -
               median(log(FL.nest.counts$distance)),
             FL.nest.counts$days_week - 
               median(FL.nest.counts$days_week),
             FL.nest.counts$days_year - 
               median(FL.nest.counts$days_year),
             0)

#jm <- list(length = length(out.names))
loo.out <- list()
Rmax <- vector(mode = "numeric", length = length(out.names))

for (k in 1:length(out.names)){
  MCMC.params$model.file = paste0("models/Model_JAGS_r2BeachSlopes_rInt_",
                                  model.names[k], ".txt")
  FL.jags.data$X <- X.FL[[k]]

  if (!file.exists(paste0("RData/JAGS_out_r2BeachSlopes_rInt_", 
                          out.names[k], "_FL.rds"))){
    jm <- jags(data = FL.jags.data,
               #inits = inits,
               parameters.to.save= parameters,
               model.file = MCMC.params$model.file,
               n.chains = MCMC.params$n.chains,
               n.burnin = MCMC.params$n.burnin,
               n.thin = MCMC.params$n.thin,
               n.iter = MCMC.params$n.samples,
               DIC = T, 
               parallel=T)
    
    saveRDS(jm, 
            file = paste0("RData/JAGS_out_r2BeachSlopes_rInt_", 
                          out.names[k], "_FL.rds"))
    
  } else {
    jm <- readRDS(file = paste0("RData/JAGS_out_r2BeachSlopes_rInt_",
                                out.names[k], "_FL.rds"))
  }

  Rmax[k] <- max(unlist(lapply(jm$Rhat, FUN = max)))
  
  if (!file.exists(paste0("RData/LOOIC_r2BeachSlopes_rInt_", 
                          out.names[k], "_FL.rds"))){
    loo.out[[k]] <- compute.LOOIC(loglik = jm$sims.list$loglik, 
                                  data.vector = FL.jags.data$count, 
                                  MCMC.params = MCMC.params)
    saveRDS(loo.out[[k]], 
            file = paste0("RData/LOOIC_r2BeachSlopes_rInt_",
                          out.names[k], "_FL.rds"))
    
  } else {
    loo.out[[k]] <- readRDS(file = paste0("RData/LOOIC_r2BeachSlopes_rInt_",
                                          out.names[k], "_FL.rds"))
  }
  
}

loo.all <- lapply(loo.out, FUN = function(X) X$loo.out) %>%
  lapply(FUN = function(X){ 
    X$estimates %>% 
      data.frame() %>% 
      rownames_to_column(var = "Statistic")}) %>%
  lapply(FUN = function(X) filter(X, Statistic == "looic") 
         %>% select(Estimate)) %>%
  unlist()

looic.df.Poi <- data.frame(ID = seq(1, length(model.names)),
                           model = model.names,
                           cov = out.names,
                           looic = loo.all,
                           MaxRhat = Rmax) %>%
  arrange(-desc(looic))



# summary(jm.FL.3Covs)
# rm(jm.FL.3Covs)
```

Then... use LOOIC or DIC to compare these models. 

```{r}
looic.df.Poi
```

Max Rhat values seem a bit high for many. The logD + DayYr model fitted best. 

```{r}
jm <- readRDS(file = paste0("RData/JAGS_out_r2BeachSlopes_rInt_",
                                out.names[looic.df.Poi[1,"ID"]],
                                 "_FL.rds"))

# Make a look up table for beach name vs beach ID vs jags' beach ID

FL.nest.counts %>% select(ID, beach_f) %>%
  mutate(ID_f = as.factor(ID)) %>%
  group_by(ID_f) %>%
  arrange(ID) %>%
  filter(row_number() == 1) -> FL_beach_ID

FL_beach_ID$jagsID <- unique(FL.jags.data$beach)
mcmc_trace(jm$samples, c("a1.1[1]", "a1.2[1]", 
                         "year.change[1]", 
                         "sigma.e"))

```

Look at the data for beach 1

```{r}
beach.data <- data.frame(counts = FL.jags.data$count[FL.jags.data$beach == 1],
                         year = FL.jags.data$yearc[FL.jags.data$beach == 1] + min(median.yr$year))

ggplot(data = beach.data) + 
  geom_path(aes(x = year, y = counts))
```

It makes sense that there was no info on year.change. 

Look at other beaches that had significant changes in counts. 

```{r}
FL.nest.counts %>% filter(beach_f == "HUTCHINSON ISL (ST. LUCIE)") %>% head()
```


```{r}
ID <- 22
jagsID <- FL_beach_ID$jagsID[FL_beach_ID$ID == ID]
mcmc_trace(jm$samples, c(paste0("a1.1[", jagsID, "]"), 
                         paste0("a1.2[", jagsID, "]"), 
                         paste0("year.change[", jagsID, "]"), 
                         "sigma.e"))

```


```{r}
ID <- 22
jagsID <- FL_beach_ID$jagsID[FL_beach_ID$ID == ID]
mcmc_dens(jm$samples, c(paste0("a1.1[", jagsID, "]"), 
                         paste0("a1.2[", jagsID, "]"), 
                         paste0("year.change[", jagsID, "]"), 
                         "sigma.e"))

```


```{r}
beach.data <- data.frame(counts = FL.jags.data$count[FL.jags.data$beach == jagsID],
                         year = FL.jags.data$yearc[FL.jags.data$beach == jagsID])

# + min(median.yr$year)
ggplot(data = beach.data) + 
  geom_path(aes(x = year, y = counts))


```

Looks like the posterior for year.change is picking up a dip around year 27. 

```{r}
Jupiter_Beach_ID <- grep("JUPITER", FL_beach_ID$beach_f)
ID <- FL_beach_ID$ID[Jupiter_Beach_ID[1]]
jagsID <- FL_beach_ID$jagsID[FL_beach_ID$ID == ID]
mcmc_dens(jm$samples, c(paste0("a1.1[", jagsID, "]"), 
                         paste0("a1.2[", jagsID, "]"), 
                         paste0("year.change[", jagsID, "]"), 
                         "sigma.e"))

```


```{r}
beach.data <- data.frame(counts = FL.jags.data$count[FL.jags.data$beach == jagsID],
                         year = FL.jags.data$yearc[FL.jags.data$beach == jagsID])

# + min(median.yr$year)
ggplot(data = beach.data) + 
  geom_path(aes(x = year, y = counts))


```

Looks like the 3 covariate model is the best. Pareto k statistic isn't great though... About 34% were > 0.7. Should we be concerned?  

```{r}
sum(loo.out[[looic.df.Poi[1,"ID"]]]$loo.out$diagnostics$pareto_k > 0.7)/length(FL.jags.data$count)
```




```{r}

plot(loo.out[[looic.df.Poi[1,"ID"]]]$loo.out)

```


Not sure if I should be concerned about these or just ignore them and move on... 

Make plots between observed and predicted for all beaches.


```{r}
jm <- readRDS(file = paste0("RData/JAGS_out_r2BeachSlopes_rInt_",
                                out.names[looic.df.Poi[1,"ID"]],
                                 "_FL.rds"))

mcmc_trace(jm$samples, c("a1.1[1]", "a1.2[1]", 
                         "year.change[1]", 
                         "sigma.e"))
```



